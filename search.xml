<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Hello World</title>
      <link href="/blog-zh/undefined/zh-CN/hello-world.html"/>
      <url>/blog-zh/undefined/zh-CN/hello-world.html</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>面孔变迁动画制作</title>
      <link href="/blog-zh/zh-CN/maker/%E9%9D%A2%E5%AD%94%E5%8F%98%E8%BF%81%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C.html"/>
      <url>/blog-zh/zh-CN/maker/%E9%9D%A2%E5%AD%94%E5%8F%98%E8%BF%81%E5%8A%A8%E7%94%BB%E5%88%B6%E4%BD%9C.html</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>昨日看见<a href="https://mp.weixin.qq.com/s/CyS6duewqNsTzP-my1cKKw">卿工</a>发出的一篇文章“<a href="https://mp.weixin.qq.com/s/CyS6duewqNsTzP-my1cKKw">动态展示孩子的相貌变化</a>”，感觉很有意思。遂到github上面拉下来代码试了试。本文是一片教程，附带一些避免坑的注意事项。</p><blockquote><p>项目地址：<a href="https://github.com/andrewdcampbell/face-movie">https://github.com/andrewdcampbell/face-movie</a></p></blockquote><p>首先上图看看这个项目的目标吧：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmdx1ftg30gh07he86.gif" alt="face-demo"></p><p>对的！通过面部识别来进行面部位置重合处理，然后进行插帧来做出渐变动画，最后可以配上音乐（可选）。</p><p>那么，我们开始吧！</p><hr><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><p>按照github代码库中描述：</p><blockquote><h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><ul><li>OpenCV<ul><li>For conda users, run <code>conda install -c conda-forge opencv</code>.</li></ul></li><li>Dlib<ul><li>For conda users, run <code>conda install -c conda-forge dlib</code>.</li></ul></li><li>ffmpeg<ul><li>For conda users, run <code>conda install -c conda-forge ffmpeg</code>.</li></ul></li><li>scipy</li><li>numpy</li><li>matplotlib</li><li>pillow</li></ul></blockquote><p>注意！这儿有个问题，由于代码是2019年版本，所以新的opencv库无法正常显示，应该安装<code>opencv-python==3.4.5.20</code>版本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opencv-python==<span class="number">3.4</span><span class="number">.5</span><span class="number">.20</span></span><br></pre></td></tr></table></figure><hr><h2 id="图片位置、大小修改"><a href="#图片位置、大小修改" class="headerlink" title="图片位置、大小修改"></a>图片位置、大小修改</h2><p>首先，新建一张画布，大小为你的目标大小，并将一张大头照粘贴到你想要的位置：如下图</p><img src="./MD.assets/BASE_IMAGE.jpg" alt="BASE_IMAGE" style="zoom: 33%;" /><p>这张图片就会成为所有图片的位置与大小的参照——其它图片会按照这张图片调整方向，大小。设这张图片为<code>参考图.jpg</code>。</p><hr><p><strong>命令行：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python face-movie/align.py -images &quot;./原始图片文件夹&quot; -target &quot;./参考图.jpg&quot; -overlay -border 5 -outdir &quot;./修改后输出图片文件夹&quot;</span><br></pre></td></tr></table></figure><p>运行时可能要你去辨别哪张脸是目人物，<code>terminal</code>如下所示：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmetb9yj31fk0aktmx.jpg" alt="image-20200428135618747"></p><p>弹出的窗口如下：（这个地方如果opencv版本不对，会显示异常）</p><img src="./MD.assets/image-20200428135623427.png" alt="image-20200428135623427" style="zoom: 25%;" /><hr><p><strong>结果：</strong></p><p>在运行后，你可以对比原始图片与结果图片：</p><blockquote><p><strong>原始图片：</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmggyiwj30uw0i07b0.jpg" alt="image-20200428140559487"></p></blockquote><blockquote><p><strong>结果图片：</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmh0xv5j30va0icdir.jpg" alt="image-20200428140635735"></p></blockquote><hr><h2 id="制作插帧影片"><a href="#制作插帧影片" class="headerlink" title="制作插帧影片"></a>制作插帧影片</h2><p><strong>命令行：</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python face-movie/main.py -morph -images &quot;./修改后输出图片文件夹&quot; -td 1.0 [转场动画秒数] -pd 2.0 [图片静止秒数] -fps 24 [帧率] -out 目标视频文件.mp4</span><br></pre></td></tr></table></figure><hr><p><strong>结果：</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmdx1ftg30gh07he86.gif" alt="face-demo"></p><hr>]]></content>
      
      
      <categories>
          
          <category> maker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
            <tag> FaceDetection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习打标签演化的Flow推荐模型</title>
      <link href="/blog-zh/zh-CN/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%89%93%E6%A0%87%E7%AD%BE%E6%BC%94%E5%8C%96%E7%9A%84flow%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B.html"/>
      <url>/blog-zh/zh-CN/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%89%93%E6%A0%87%E7%AD%BE%E6%BC%94%E5%8C%96%E7%9A%84flow%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmkseryj31bk0imjtf.jpg" alt="inline"></p><p>基于OpenML框架的flow推荐</p><hr><h2 id="各种不同任务的深度学习标签的例子"><a href="#各种不同任务的深度学习标签的例子" class="headerlink" title="各种不同任务的深度学习标签的例子"></a>各种不同任务的深度学习标签的例子</h2><p><a href="https://github.com/kyungyunlee/sampleCNN-pytorch">音乐</a>: Pytorch implementation of “Sample-level Deep Convolutional Neural Networks for Music Auto-tagging Using Raw Waveforms”</p><p><a href="https://github.com/kyungyunlee/sampleCNN-pytorch">词性</a>: 使用CoNLL-2012数据集的一部分和42个可能的标记来实现RNN并比较不同类型的RNN的词性标记任务。</p><p><a href="https://github.com/SendongZhao/RNN-for-tagging">RNN-for-tagging</a>: Pytorch implementation of LSTM, Bi-LSTM and Bi-LSTM-CRF which are in the context of NER.</p><p><a href="https://github.com/Emrys-Hong/fastai_sequence_tagging">命名实体识别</a>（NER）：sequence tagging for NER for ULMFiT</p><p><a href="https://github.com/akurniawan/pytorch-sequence-tagger">Sequence Tagger implementation</a>：build model that is used for sequence tagging (POS Tag, NER, etc.). </p><p><a href="https://medium.com/tensorflow/building-a-text-classification-model-with-tensorflow-hub-and-estimators-3169e7aa568">电影多标签分类器</a>：使用TensorFlow Hub和Estimators构建文本分类模型</p><p><a href="https://www.dlology.com/blog/how-to-do-multi-class-multi-label-classification-for-news-categories/">新闻话题地点多标签分类</a>：With a given news, our task is to give it one or multiple tags. The dataset is divided into five main categories.</p><p><a href="https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/">鸢尾花分类</a>：多标签图片分类。标签总量少。</p><hr><h2 id="相关技术"><a href="#相关技术" class="headerlink" title="相关技术"></a>相关技术</h2><p><a href="https://www.jianshu.com/p/0fb6c9886542">ULMFiT</a>-用于文本分类的通用语言模型微调：一种可以在任何自然语言处理任务上实现类似CV的转移学习的方法。</p><p><a href="https://www.jianshu.com/p/16e1f6a7aaef">命名实体识别</a>（NER）：是信息提取、问答系统、句法分析、机器翻译等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。一般来说，命名实体识别的任务就是识别出待处理文本中三大类（实体类、时间类和数字类）、七小类（人名、机构名、地名、时间、日期、货币和百分比）命名实体。</p><p>自然语言处理（NLP）</p><p><a href="https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/">多标签文本分类</a>：用于提取社交网络上的对话是否含有辱骂、威胁、赞扬等信息。</p><hr><h2 id="再次重复说明OpenML构造"><a href="#再次重复说明OpenML构造" class="headerlink" title="再次重复说明OpenML构造"></a>再次重复说明OpenML构造</h2><p>每个data对应多个task：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmm3inmj31c409o408.jpg" alt="inline"></p><p>每个task对应一个data：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmmmhkaj314i07ywfy.jpg" alt="inline"></p><p>每个task对应多个flow，每个flow对应多个run:</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmnzuaaj31yu0jc79w.jpg" alt="inline"></p><p>每个run对应data+task+flow</p><hr><h2 id="用图片说明OpenML构造"><a href="#用图片说明OpenML构造" class="headerlink" title="用图片说明OpenML构造"></a>用图片说明OpenML构造</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmoiv6rj31lr0tcq6e.jpg" alt="inline"></p><hr><h2 id="任务说明"><a href="#任务说明" class="headerlink" title="任务说明"></a>任务说明</h2><p>为了简化过程，暂时使用以下方法：</p><ul><li>由于每个data所对应的多个task中，排名第一的往往比第二的运行次数多很多，所以取第一个为最佳task。</li><li>然后在选中的task中选取前n名为flow。</li></ul><p><strong>详细说明如下：</strong></p><hr><h2 id="数据集分析"><a href="#数据集分析" class="headerlink" title="数据集分析"></a>数据集分析</h2><ul><li>数据集的介绍</li></ul><p>忽略自带的tag，无用。</p><p>数据库介绍使用文本来找出task-type。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmpf6u6j31by0kiwiv.jpg" alt="inline"></p><hr><h2 id="数据集分析-1"><a href="#数据集分析-1" class="headerlink" title="数据集分析"></a>数据集分析</h2><ul><li>数据集的特征</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmpvuicj313p0u0dk4.jpg" alt="inline"></p><hr><h2 id="data中选取task的方法"><a href="#data中选取task的方法" class="headerlink" title="data中选取task的方法"></a>data中选取task的方法</h2><p>对于不达标的task，忽略，防止训练内容有误。</p><p>此处选择 $n = 1$。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmss2oij31c80pmtet.jpg" alt="inline"></p><hr><h2 id="task中选取flow方法"><a href="#task中选取flow方法" class="headerlink" title="task中选取flow方法"></a>task中选取flow方法</h2><p>按照精度排序（这儿假设performance就是accuracy）选取前n个。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmttirgj31dx0u0tf3.jpg" alt="inline"></p><hr><h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p>由于我们的task是不能互通的，每个task必须针对特定的data生成。所以我们不能推荐task，而是推荐task-type。这个之前已经成功实现且acc&gt;80%。所以此处忽略。</p><p>此处仅仅训练data推荐flow：训练集为<br>（data的properties）：(flow)</p><blockquote><p>我只做了一个小的csv数据集，内容为</p><p>dataid-properties-flow1-flow2-…-flow5</p><p>共50条用来测试。</p></blockquote><hr><h2 id="机器学习算法-暂时pass"><a href="#机器学习算法-暂时pass" class="headerlink" title="机器学习算法(暂时pass)"></a>机器学习算法(暂时pass)</h2><p>2.3.3 用于多分类的线性模型</p><p>将二分类算法推广到多分类算法的一种常见方法是“一对其余”(one-vs.-rest)方<br>法。</p><p>然而，flow数量很多，二分类算法运行效果的双边基数相差太大，用线性二分类难以区分。</p><p>2.3.6 决策树</p><p>对于维度非常高的稀疏数据(比如文本数据)，随机森林的表现往往不是很好。对于这种数据，使用线性模型可能更合适。</p><p>即使是非常大的数据集，随机森林的表现通常也很好！不过，随机森林需要更大的内存，训练和预测的速度也比线性模型要慢。</p><hr><h2 id="单标签任务-iris数据集示范-改写失败"><a href="#单标签任务-iris数据集示范-改写失败" class="headerlink" title="单标签任务(iris数据集示范-改写失败)"></a>单标签任务(iris数据集示范-改写失败)</h2><p>参考书籍-其它<a href="https://www.kaggle.com/kernels/scriptcontent/1380110/download">网页</a>：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmuz7foj30dw0i8jt9.jpg" alt="inline"></p><ul><li>使用<code>特征值-分类</code>方法，由于之前爬取数据太全面，导致速度慢，爬取数据量少。全部数据仅有50个data和20000个flow。</li><li>由于数据量少，而标签（flow）数量是data的很多倍。</li><li>且由于单标签，不能用于flow推荐算法。</li></ul><hr><h2 id="任务构造-理想"><a href="#任务构造-理想" class="headerlink" title="任务构造(理想)"></a>任务构造(理想)</h2><blockquote><p>参考：<a href="https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/">Guide To Multi-Class Multi-Label Classification With Neural Networks In Python</a></p></blockquote><ol><li>将数据集看做需要打标签的目标；</li><li>将flow的id看做标签；</li><li>首先将数据集的特征值归一化；</li><li>然后类似文本分类(文本分类是单词存在标1，否则0)，将特征看做文本单词，权重[0，1]；与特征的顺序无关（特征没有前后之分）;</li><li>挑出所有用到的flow作为标签, <strong>将标签转换为多热编码</strong><br>flows = [ flow1, flow2, flow3, … ]<br>[ 0  0  0  1  0  0  0  1  0 … ]</li><li>进行数据特征学习分类。</li></ol><hr><h2 id="为什么任然不能使用？"><a href="#为什么任然不能使用？" class="headerlink" title="为什么任然不能使用？"></a><strong>为什么任然不能使用？</strong></h2><p>由于文本和特征值还是有很大区别。</p><p>这个算法可以用来进行<strong>data的描述分辨tasktype</strong>。</p><hr><h2 id="与常规任务比较分析"><a href="#与常规任务比较分析" class="headerlink" title="与常规任务比较分析"></a>与<a href="https://cloud.tencent.com/developer/article/1169364">常规任务</a>比较分析</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmvh2cbj30go09s75j.jpg" alt="inline"></p><ol><li>常规多标签任务标签量少：比如对 iris flower 分类，只有十种。若是类似识别任务的种类多，但是不是多标签，是单标签。</li><li>常规多标签任务标签不分先后：比如-红色-衣服-有口袋 这种，不分先后。</li><li>常规多标签任务基于图片分析，或者是长段文字分析。</li></ol><hr><h2 id="不足之处的解决或规避方法"><a href="#不足之处的解决或规避方法" class="headerlink" title="不足之处的解决或规避方法"></a>不足之处的解决或规避方法</h2><p>由于推荐的目标是达到最高性能，所以忽略了人的信息。暂时不管。</p><p>当前挑出所有用到的flow作为标签, <strong>将标签转换为多热编码</strong><br>flows = [ flow1, flow2, flow3, … ]<br>[ 0  0  0  1  0  0  0  1  0 … ]<br>这样不好，丢失了排序信息。所以我们应该使用小数编码，将性能表现为0-1之间的小数，[ 0.988  0.983  0.970  0.838 … ]</p><hr><h2 id="接下来怎么办？"><a href="#接下来怎么办？" class="headerlink" title="接下来怎么办？"></a>接下来怎么办？</h2><p>希望可以找到与我的openml数据集类似的数据集。</p><p>现在测试过：</p><ul><li>基于图片的分类</li><li>图片的多标签任务</li><li>基于文本(一长段文字)的多标签</li><li>基于数据特征(iris)的分类</li></ul><p>希望找到：</p><ul><li>基于数据特征的多标签</li></ul>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于深度学习的推荐系统</title>
      <link href="/blog-zh/zh-CN/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html"/>
      <url>/blog-zh/zh-CN/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F.html</url>
      
        <content type="html"><![CDATA[<h1 id="Deep-Learning-based-Recommender-System"><a href="#Deep-Learning-based-Recommender-System" class="headerlink" title="Deep Learning based Recommender System"></a>Deep Learning based Recommender System</h1><blockquote><p>本文的概念主要内容来自：arXiv:1707.07435v6</p><ul><li>一个电影推荐系统（基于tensorflow）示例的运行。</li><li>一个文本深度学习的电影推荐系统，包含<strong>看过这个的人也看过</strong>，<strong>和你一样的人看过</strong>（猜你喜欢）。</li></ul></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscvdnhhsj306q084wen.jpg" alt="img"><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscvkq1n2j30680840sv.jpg" alt="img"><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscvm6txvj30840a0jrv.jpg" alt="img"></p><hr><h2 id="contributions-of-the-survey"><a href="#contributions-of-the-survey" class="headerlink" title="contributions of the survey"></a>contributions of the survey</h2><p>(1) We conduct a <strong>systematic review</strong> for recommendation models based on deep learning techniques and propose a <strong>classification scheme</strong> to position and organize the current work; </p><p>(2) We provide an overview and summary for the state-of-the-arts. 提供现有技术的概述和总结</p><p>(3) We discuss the challenges and open issues, and identify the new trends and future directions in this research field to share the vision and expand the horizons of deep learning based recommender system research.</p><hr><h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><ul><li>the preliminaries for recommender systems and deep neural networks</li><li>the advantages and disadvantages of deep neural network based recommendation models</li><li>presents our classification framework</li><li>gives detailed introduction to the state-of-the-art</li><li>challenges and prominent open research issues</li></ul><hr><h2 id="Recommendation-models"><a href="#Recommendation-models" class="headerlink" title="Recommendation models"></a>Recommendation models</h2><p>classified into three categories [1, 69]: </p><ul><li>collaborative filtering, <ul><li>通过学习用户项目历史交互来提出建议，无论是显式（例如用户先前的评级）还是隐式反馈（例如浏览历史）</li></ul></li><li>content based <ul><li>基于项目和用户之间的比较 (加上辅助信息，如文本，图像)</li></ul></li><li>hybrid recommender system. <ul><li>混合</li></ul></li></ul><hr><h2 id="Deep-Learning-Techniques"><a href="#Deep-Learning-Techniques" class="headerlink" title="Deep Learning Techniques"></a>Deep Learning Techniques</h2><p>we consider any neural differentiable architecture as ‘deep learning‘ as long as it optimizes a differentiable objective function using a variant of stochastic gradient descent (SGD)</p><hr><h2 id="Deep-Learning-Techniques-1"><a href="#Deep-Learning-Techniques-1" class="headerlink" title="Deep Learning Techniques"></a>Deep Learning Techniques</h2><h3 id="rchitectural-paradigms"><a href="#rchitectural-paradigms" class="headerlink" title="rchitectural paradigms"></a>rchitectural paradigms</h3><ul><li>多层感知器（MLP）</li><li>[自动编码器（AE）](file:///Users/bingcheng/Google 云端硬盘/论文/deep learning recommendation/2018 Deep Learning based Recommender System- A Survey and New Perspectives.html#bookmark64)</li><li>[递归神经网络（RNN）](file:///Users/bingcheng/Google 云端硬盘/论文/deep learning recommendation/2018 Deep Learning based Recommender System- A Survey and New Perspectives.html#bookmark94)</li><li>受限玻尔兹曼机（RBM）</li><li>[神经自回归分布估计（NADE）](file:///Users/bingcheng/Google 云端硬盘/论文/deep learning recommendation/2018 Deep Learning based Recommender System- A Survey and New Perspectives.html#bookmark130)</li><li>[对抗网络（AN）](file:///Users/bingcheng/Google 云端硬盘/论文/deep learning recommendation/2018 Deep Learning based Recommender System- A Survey and New Perspectives.html#bookmark95)</li><li>质量模型（AM）</li><li>[深层强化学习（DRL）](file:///Users/bingcheng/Google 云端硬盘/论文/deep learning recommendation/2018 Deep Learning based Recommender System- A Survey and New Perspectives.html#bookmark155)</li></ul><hr><h2 id="Why-Deep-Neural-Networks-for-Recommendation"><a href="#Why-Deep-Neural-Networks-for-Recommendation" class="headerlink" title="Why Deep Neural Networks for Recommendation?"></a>Why Deep Neural Networks for Recommendation?</h2><p>这里的关键优势是处理<em>基于内容的</em>推荐。</p><p>例如，在处理文本数据，图像数据（社交帖子，产品图像）时，CNN / RNN成为不可或缺的神经构建块。传统的替代方案（设计模态特定的特征等）要处理评论，必须执行<em>昂贵</em>的预处理（例如，关键短语提取，主题建模等），而较新的基于深度学习的方法能够端到端地摄取所有文本信息[202]。</p><p>总而言之，深度学习的在这方面的能力卓越，不用预处理，计算量小（提前需要进行训练的量大），可以瞬时推荐，但是这里没有说准确度等我们关系的问题，后面应该会讲到。</p><hr><h2 id="Summarize-the-strengths-of-deep-learning-based-recommendation-models"><a href="#Summarize-the-strengths-of-deep-learning-based-recommendation-models" class="headerlink" title="Summarize the strengths of deep learning based recommendation models"></a>Summarize the strengths of deep learning based recommendation models</h2><ul><li><p>非线性变换</p><p><em>该属性使得处理复杂的交互能够成为可能并精确地反映用户的偏好。</em></p></li><li><p>表征学习</p><ol><li>它减少了手工调参。调参是一项劳动密集型工作，深度神经网络能够在无人监督或监督的方法中自动从原始数据中学习特征；</li><li>它使推荐模型能够包括异构内容信息，如文本，图像，音频甚至视频。</li></ol></li><li><p>序列建模</p><p>挖掘用户行为和时间演变之间的关系：用于机器翻译，自然语言理解，语音识别，聊天机器人等等</p></li><li><p>灵活性</p><p>框架多：Tensorflow , Keras, Caffe, MXnet, DeepLearning4j , PyTorch, Theano…</p></li></ul><hr><h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><ul><li><p>隐藏的权重和激活通常是不可解释的，限制了可解释性。</p></li><li><p>需要进行广泛的超参数调整</p><p>最近的一项工作[145]，只引入了一个超参数。</p></li></ul><hr><h2 id="基于深度学习的推荐模型"><a href="#基于深度学习的推荐模型" class="headerlink" title="基于深度学习的推荐模型"></a>基于深度学习的推荐模型</h2><ol><li>类别</li><li>最先进的技术</li></ol><hr><h3 id="基于深度学习的推荐模型的类别"><a href="#基于深度学习的推荐模型的类别" class="headerlink" title="基于深度学习的推荐模型的类别"></a>基于深度学习的推荐模型的类别</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscvnhv8fj30dc03k3yn.jpg" alt="img"></p><hr><h2 id="任务分类"><a href="#任务分类" class="headerlink" title="任务分类"></a>任务分类</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscvpyyrsj31d60qijwy.jpg" alt="img"></p><hr><h2 id="模型分类"><a href="#模型分类" class="headerlink" title="模型分类"></a>模型分类</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscvx0mfij312k0mg78z.jpg" alt="img"></p><hr><p>接下来论文花了十多页将各个模型分别介绍。</p><p>略过难以借鉴的。</p><p>MLP：Multilayer Perceptron based Recommendation，多层感知器</p><p>可用于向现有RS方法添加非线性变换，并将其解释为神经扩展。</p><ol><li><em>NCF Neural Collaborative Filtering 神经协同过滤</em></li><li>Deep Factorization Machine. DeepFM.<em>深度分解机</em></li><li>…</li></ol><p><strong>过于繁杂难以理解</strong></p><hr><h2 id="未来的深度推荐发展方向"><a href="#未来的深度推荐发展方向" class="headerlink" title="未来的深度推荐发展方向"></a>未来的深度推荐发展方向</h2><ul><li><p>提高可解释性</p></li><li><p>更深入</p></li><li><p>跨域推荐</p></li><li><p>多任务学习</p></li><li><p>可扩展性</p><p>(无需掌握，略过)</p></li></ul><hr><h2 id="fit-深度学习实践"><a href="#fit-深度学习实践" class="headerlink" title="[fit]深度学习实践"></a>[fit]深度学习实践</h2><p>一个电影推荐系统（基于tensorflow）示例的运行。 一个文本深度学习的电影推荐系统，包含<strong>看过这个的人也看过</strong>，<strong>和你一样的人看过</strong>（猜你喜欢）。</p><hr><h2 id="手写数字识别的神经网络"><a href="#手写数字识别的神经网络" class="headerlink" title="手写数字识别的神经网络"></a>手写数字识别的神经网络</h2><p>书籍：Make Your Own Neural Network - 2016</p><p>主要是学习神经网络相关知识，运行了示例程序。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscw1lipoj311u0pwjut.jpg" alt="img"></p><hr><h2 id="fit-神经网络基础概念"><a href="#fit-神经网络基础概念" class="headerlink" title="[fit]神经网络基础概念"></a>[fit]神经网络基础概念</h2><p>主要来自：Make Your Own Neural Network</p><p>不清楚的搜了百度的解释。</p><hr><h2 id="1-神经元"><a href="#1-神经元" class="headerlink" title="1. 神经元"></a>1. 神经元</h2><p>正如神经元是大脑的基本单位一样，在神经网络结构中，神经元也是一个小单位。大家不妨想象一下当我们接触到新的信息时，大脑是如何运作的。</p><p>首先，我们会在脑中处理这个信息，然后产生输出信息。在神经网络中也是如此，<strong>神经元接收到一个输入信息，然后对它进行加工处理，然后产生输出信息，传输到其他神经元中进行进一步信息处理。</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscw35f3ij30lc0dcwf2.jpg" alt="img"></p><hr><h2 id="2-权重"><a href="#2-权重" class="headerlink" title="2. 权重"></a>2. 权重</h2><p><strong>当输入信息到达神经元时，它就会乘上一个权重。</strong>举例来说，如果一个神经元包含两个输入信息，那么每个输入信息都被赋予它的关联权重。我们随机初始化权重，并在模型训练过程中更新这些权重。</p><p>接受训练后的神经网络会赋予它认为重要的输入信息更高的权重值，而那些不重要的输入信息权重值则会相对较小。权重值为零就意味着这个特征是无关紧要的。</p><p>我们不妨假设输入信息为 a，其关联权重为 W1, 通过节点后，输入信息变为 a*W1：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscw8p9s7j3071053dfq.jpg" alt="img"></p><hr><h2 id="3-偏置"><a href="#3-偏置" class="headerlink" title="3. 偏置"></a>3. 偏置</h2><p><strong>除了权重之外，输入还有另一个线性分量，被称为偏置。</strong>输入信息乘上权重后再加上偏置，用来改变权重乘输入的范围。加上偏置之后，结果就变为 a*W1+bias，这就是输入信息变换的最终线性分量。</p><hr><h2 id="4-激活函数"><a href="#4-激活函数" class="headerlink" title="4. 激活函数"></a>4. 激活函数</h2><p>**线性分量应用可以到输入信息，非线性函数也可以应用到输入信息。这种输入信息过程是通过激活函数来实现的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscwaxyb7j308c05f74b.jpg" alt="img"></p><p>最常用的激活函数有 Sigmoid、ReLU 和 softmax。</p><hr><ul><li><strong>Sigmoid</strong>——Sigmoid 是最常用的激活函数之一。它的定义为：</li></ul><p>sigmoid(x) = 1/(1+e-x)</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscwijjvhj307n053q2u.jpg" alt="img"></p><p>Sigmoid 函数会生成 0 到 1 之间的更平滑的取值范围。我们可能需要观察输出值的变化，同时输入值也会略有变化。而平滑的曲线更方便我们观察，因此它优于阶梯函数（step functions）。</p><hr><ul><li><strong>ReLU（线性修正单位）</strong>——不同于 Sigmoid 函数，现在的网络更倾向于使用隐层 ReLu 激活函数。该函数的定义是：</li></ul><p>f(x) = max(x,0)</p><p>当 X&gt;0 时，函数的输出为 X，当 X&lt;=0 时为 0。该函数如下所示：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscwq36hpj308c05nwed.jpg" alt="img"></p><p>使用 ReLu 的好处主要是它对于大于 0 的所有输入值都有对应的不变导数值。而常数导数值可以加快对网络的训练。</p><hr><ul><li><strong>Softmax</strong>——Softmax 激活函数常用于输出层的分类问题。它与 Sigmoid 函数类似，唯一的区别是在 Softmax 激活函数中，输出被归一化，总和变为 1。</li></ul><p>如果我们遇到的是二进制输出问题，就可以使用 Sigmoid 函数，而如果我们遇到的是多类型分类问题，使用 Softmax 函数可以轻松地为每个类型分配值，并且可以很容易地将这个值转化为概率。</p><p>这样看可能更容易理解一些——假设你正在尝试识别一个看起来像 8 实际为 6 的数。该函数将为每个数字赋值，如下所示。我们可以很容易地看出，最高概率被分配给了 6，下一个最高概率则分配给 8，依此类推……</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscwv7zryj308c03swef.jpg" alt="img"></p><hr><h2 id="5-神经网络"><a href="#5-神经网络" class="headerlink" title="5. 神经网络"></a>5. 神经网络</h2><p>神经网络是深度学习的主干之一。<strong>神经网络的目标是找到未知函数的一个近似值。它由相互联系的神经元组成。</strong></p><p>这些神经元具有权重，并且会根据出错情况，在网络训练期间更新偏置值。激活函数将非线性变换置于线性组合，之后生成输出。被激活的神经元组合再产生输出。</p><p>对神经网络的定义中，以 Liping Yang 的最为贴切：</p><blockquote><p>神经网络由许多相互关联的概念化的人造神经元组成，这些人造神经元之间可以互相传递数据，并且具有根据网络「经验」调整的相关权重。</p><p>神经元具有激活阈值，如果结合相关权重组合并激活传递给他们的数据，神经元的激活阈值就会被解除，激活的神经元的组合就会开始「学习」。</p></blockquote><hr><h2 id="6-输入层-输出层-隐藏层"><a href="#6-输入层-输出层-隐藏层" class="headerlink" title="6. 输入层 / 输出层 / 隐藏层"></a>6. 输入层 / 输出层 / 隐藏层</h2><p>顾名思义，输入层是接收输入信号的一层，也是该网络的第一层；输出层则是传递输出信号的一层，也是该网络的最后一层。</p><p>处理层则是该网络中的「隐含层」。<strong>这些「隐含层」将对输入信号进行特殊处理，并将生成的输出信号传递到下一层。</strong>输入层和输出层均是可见的，而其中间层则是隐藏起来的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscwxedodj309x04iglq.jpg" alt="img"></p><hr><h2 id="7-MLP（多层神经网络）"><a href="#7-MLP（多层神经网络）" class="headerlink" title="7. MLP（多层神经网络）"></a>7. MLP（多层神经网络）</h2><p>单一神经元无法执行高度复杂的任务。因此，需要大量神经元聚集在一起才能生成我们所需要的输出信号。</p><p>最简单的网络由一个输入层、一个输出层、一个隐含层组成，每一层上都有多个神经元，并且每一层上的神经元都和下一层上的神经元连接在了一起，这样的网络也被称为全互连网络（Fully Connected Networks）。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscwzpwa6j30lc0ah3zj.jpg" alt="img"></p><hr><h2 id="8-正向传播（Forward-Propagation）"><a href="#8-正向传播（Forward-Propagation）" class="headerlink" title="8. 正向传播（Forward Propagation）"></a>8. 正向传播（Forward Propagation）</h2><p><strong>正向传播指的是输入信号通过隐藏层传递到输出层的传递过程。</strong></p><p>在正向传播中，信号仅沿单一方向向前正向传播，输入层将输入信号提供给隐藏层，隐藏层生成输出信号，这一过程中没有任何反向移动。</p><hr><h2 id="9-成本函数（Cost-Function）"><a href="#9-成本函数（Cost-Function）" class="headerlink" title="9. 成本函数（Cost Function）"></a>9. 成本函数（Cost Function）</h2><p>当我们建立一个网络后，网络将尽可能地使输出值无限接近于实际值。<strong>我们用成本函数（或损失函数）来衡量该网络完成这一过程的准确性。</strong>成本函数（或损失函数）将在该网络出错时，予以警告。</p><p>运行网络时，我们的目标是：<strong>尽可能地提高我们的预测精度、减少误差，由此最小化成本函数。</strong>最优化的输出即当成本函数（或损失函数）为最小值时的输出。</p><p>若将成本函数定义为均方误差，则可写成：</p><p>C= 1/m ∑(y – a)2</p><p>m 在这里是训练输入值（Training Inputs），a 是预计值，y 是特定事例中的实际值。</p><p>学习过程围绕着如何最小化成本。</p><hr><h2 id="10-梯度下降（Gradient-Descent）"><a href="#10-梯度下降（Gradient-Descent）" class="headerlink" title="10. 梯度下降（Gradient Descent）"></a>10. 梯度下降（Gradient Descent）</h2><p><strong>梯度下降是一种成本最小化的优化算法。</strong>想象一下，当你下山时，你必须一小步一小步往下走，而不是纵身一跃跳到山脚。</p><p>因此，我们要做的是：比如，我们从 X 点开始下降，我们下降一点点，下降 ΔH，到现在的位置，也就是 X-ΔH，重复这一过程，直到我们到达「山脚」。「山脚」就是最低成本点。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscx1m8nuj30jj0a6dgl.jpg" alt="img"></p><p>从数学的角度来说，要找到函数的局部极小值，须采取与函数梯度负相关的「步子」，即：梯度下降法是用负梯度方向为搜索方向的，梯度下降法越接近目标值，步长越小，前进越慢。</p><hr><h2 id="11-学习速率（Learning-Rate）"><a href="#11-学习速率（Learning-Rate）" class="headerlink" title="11. 学习速率（Learning Rate）"></a>11. 学习速率（Learning Rate）</h2><p><strong>学习速率指每次迭代中对成本函数的最小化次数。</strong>简单来说，我们把下降到成本函数最小值的速率称为学习率。选择学习率时，我们必须非常小心，学习速率既不应过大——会错过最优解，也不应过小——使网络收敛将需要很多很多步、甚至永不可能。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscx355d3j30cr0bi0t4.jpg" alt="img"></p><hr><h2 id="12-反向传播（Back-Propagation）"><a href="#12-反向传播（Back-Propagation）" class="headerlink" title="12. 反向传播（Back Propagation）"></a>12. 反向传播（Back Propagation）</h2><p>在定义一个神经网络的过程中，每个节点会被随机地分配权重和偏置。</p><p>一次迭代后，我们可以根据产生的结果计算出整个网络的偏差，然后用偏差结合成本函数的梯度，对权重因子进行相应的调整，使得下次迭代的过程中偏差变小。</p><p><strong>这样一个结合成本函数的梯度来调整权重因子的过程就叫做反向传播。</strong>在反向传播中，信号的传递方向是朝后的，误差连同成本函数的梯度从输出层沿着隐藏层传播，同时伴随着对权重因子的调整。</p><hr><h2 id="13-分批（Batches）"><a href="#13-分批（Batches）" class="headerlink" title="13. 分批（Batches）"></a>13. 分批（Batches）</h2><p>当我们训练一个神经网路时，我们不应一次性发送全部输入信号，而应把输入信号随机分成几个大小相同的数据块发送。</p><p>与将全部数据一次性送入网络相比，在训练时将数据分批发送，建立的模型会更具有一般性。</p><hr><h2 id="14-周期（Epochs）"><a href="#14-周期（Epochs）" class="headerlink" title="14. 周期（Epochs）"></a>14. 周期（Epochs）</h2><p>一个周期表示对所有的数据批次都进行了一次迭代，包括一次正向传播和一次反向传播，所以<strong>一个周期就意味着对所有的输入数据分别进行一次正向传播和反向传播。</strong></p><p>训练网络周期的次数是可以选择的，往往周期数越高，模型的准确性就越高，但是，耗时往往就越长。同样你还需要考虑如果周期 / 纪元的次数过高，那么可能会出现过拟合的情况。</p><hr><h2 id="15-Dropout-方法"><a href="#15-Dropout-方法" class="headerlink" title="15. Dropout 方法"></a>15. Dropout 方法</h2><p>Dropout 是一个可以阻止网络过拟合的规则化方法。就像它的名字那样，在训练过程中隐藏的某些特定神经元会被忽略掉（drop）。</p><p>这意味着<strong>网络的训练是在几个不同的结构上完成的</strong>。这种 Dropout 的方式就像是一场合奏，多个不同结构网络的输出组合产生最终的输出结果。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscx5hy56j30lc08z0tk.jpg" alt="img"></p><hr><h2 id="16-分批标准化（Batch-Normalization）"><a href="#16-分批标准化（Batch-Normalization）" class="headerlink" title="16. 分批标准化（Batch Normalization）"></a>16. 分批标准化（Batch Normalization）</h2><p>分批标准化就像是人们在河流中用以监测水位的监察站一样。</p><p>这是为了<strong>保证下一层网络得到的数据拥有合适的分布</strong>。在训练神经网络的过程中，每一次梯度下降后权重因子都会得到改变，从而会改变相应的数据结构。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscx8rx7kj30kf0g6wfy.jpg" alt="img"></p><hr><p>但是下一层网络希望能够得到与之前分布相似的数据，因此在每一次数据传递前都需要对数据进行一次正则化处理。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscx9kyrjj30jl0fagmi.jpg" alt="img"></p><hr><h2 id="fit-卷积神经网络相关知识"><a href="#fit-卷积神经网络相关知识" class="headerlink" title="[fit]卷积神经网络相关知识"></a>[fit]卷积神经网络相关知识</h2><hr><h2 id="17-过滤器-滤波器（Filters）"><a href="#17-过滤器-滤波器（Filters）" class="headerlink" title="17. 过滤器 / 滤波器（Filters）"></a>17. 过滤器 / 滤波器（Filters）</h2><p><strong>CNN 中的滤波器，具体是指将一个权重矩阵乘以输入图像的一个部分，产生相应的卷积输出。</strong></p><p>比方说，对于一个 28×28 的图片而言，将一个 3×3 的滤波器与图片中 3×3 的矩阵依次相乘，从而得到相应的卷积输出。</p><p>滤波器的尺寸通常比原始图片要小，与权重相似，在最小化成本的反向传播中，滤波器也会被更新。就像下面这张图片一样，通过一个过滤器，依次乘以图片中每个 3×3 的分块，从而产生卷积的结果。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscxbxcg9j30lc0jyq4g.jpg" alt="img"></p><hr><h2 id="18-卷积神经网络-CNN（Convolutional-Neural-Network）"><a href="#18-卷积神经网络-CNN（Convolutional-Neural-Network）" class="headerlink" title="18. 卷积神经网络 CNN（Convolutional Neural Network）"></a>18. 卷积神经网络 CNN（Convolutional Neural Network）</h2><p><strong>卷积神经网络通常用来处理图像数据</strong>，假设输入数据的形状为 28×28×3（28pixels × 28pixels × RGBValue），那么对于传统的神经网络来说就会有 2352（28×28×3）个变量。随着图像尺寸的增加，那么变量的数量就会急剧增加。</p><p>通过对图片进行卷积，可以减少变量的数目（已在过滤器的概念中提及）。随着过滤器沿着图像上宽和高的两个方向滑动，就会产生一个相应的 2 维激活映射，最后再沿纵向将所有的激活映射堆叠在一起，就产生了最后的输出。</p><p>可以参照下面这个示意图。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscxcv1fhj30ft05mglv.jpg" alt="img"></p><hr><h2 id="19-池化（Pooling）"><a href="#19-池化（Pooling）" class="headerlink" title="19. 池化（Pooling）"></a>19. 池化（Pooling）</h2><p><strong>为进一步减少变量的数目同时防止过拟合，一种常见的做法是在卷积层中引入池化层（Pooling Layer）。</strong></p><p>最常用的池化层的操作是将原始图片中每个 4×4 分块取最大值形成一个新的矩阵，这叫做最大值池化（Max Pooling）。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscxed5e4j30l408ewf1.jpg" alt="img"></p><p>也有人尝试诸如平均池化（Average Pooling）之类的方式，但在实际情况中最大化池化拥有更好的效果。</p><hr><h2 id="20-补白（Padding）"><a href="#20-补白（Padding）" class="headerlink" title="20. 补白（Padding）"></a>20. 补白（Padding）</h2><p><strong>补白（Padding）通常是指给图像的边缘增加额外的空白，从而使得卷积后输出的图像跟输入图像在尺寸上一致，这也被称作相同补白（Same Padding）。</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscxg8tpdj306p05tjre.jpg" alt="img"></p><p>如应用过滤器，在相同补白的情况下，卷积后的图像大小等于实际图像的大小。</p><p>有效补白（Valid Padding）指的是保持图片上每个真实的像素点，不增加空白，因此在经历卷积后数据的尺寸会不断变小。</p><hr><h2 id="21-数据增强（Data-Augmentation）"><a href="#21-数据增强（Data-Augmentation）" class="headerlink" title="21. 数据增强（Data Augmentation）"></a>21. 数据增强（Data Augmentation）</h2><p><strong>数据增强指的是从已有数据中创造出新的数据，通过增加训练量以期望能够提高预测的准确率。</strong></p><p>比如，在数字识别中，我们遇到的数字 9 可能是倾斜或旋转的，因此如果将训练的图片进行适度的旋转，增大训练量，那么模型的准确性就可能会得到提高。</p><p>通过「旋转」「照亮」的操作，训练数据的品质得到了提升，这种过程被称作数据增强。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscxmcatbj30ae04fjrb.jpg" alt="img"></p><hr><h2 id="fit-递归神经网络相关知识"><a href="#fit-递归神经网络相关知识" class="headerlink" title="[fit]递归神经网络相关知识"></a>[fit]递归神经网络相关知识</h2><hr><h2 id="22-递归神经元（Recurrent-NeuralNetwork）"><a href="#22-递归神经元（Recurrent-NeuralNetwork）" class="headerlink" title="22. 递归神经元（Recurrent NeuralNetwork）"></a>22. 递归神经元（Recurrent NeuralNetwork）</h2><p><strong>对于递归神经元来说，经由它自己处理过的数据会变成自身下一次的输入，这个过程总共会进行 t 次。</strong></p><p>如下图所示，将递归神经元展开就相当于 t 个不同的神经元串联起来，这种神经元的长处是能够产生一个更全面的输出结果。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscxt9qrbj30l2050t8v.jpg" alt="img"></p><hr><h2 id="23-递归神经网络（RNN-Recurrent-NeuralNetwork）"><a href="#23-递归神经网络（RNN-Recurrent-NeuralNetwork）" class="headerlink" title="23. 递归神经网络（RNN-Recurrent NeuralNetwork）"></a>23. 递归神经网络（RNN-Recurrent NeuralNetwork）</h2><p><strong>递归神经网络通常被用于处理序列化的数据，即前一项的输出是用来预测下一项的输入。</strong></p><p>递归神经网络通常被用于处理序列化的数据，即前一项的输出是用来预测下一项的输入。递归神经网络中存在环的结构，这些神经元上的环状结构使得它们能够存储之前的数据一段时间，从而使得能够预测输出。</p><p>与递归神经元相似，在 RNN 中隐含层的输出会作为下一次的输入，如此往复经历 t 次，再将输出的结果传递到下一层网络中。这样，最终输出的结果会更全面，而且之前训练的信息被保持的时间会更久。</p><p>隐藏层将反向传递错误以更新权重。这被称为 backpropagation through time（BPTT）。</p><hr><h2 id="24-梯度消失问题"><a href="#24-梯度消失问题" class="headerlink" title="24. 梯度消失问题"></a>24. 梯度消失问题</h2><p><strong>当激活函数的梯度非常小时，会出现梯度消失问题。</strong>在反向传播过程中，权重因子会被多次乘以这些小的梯度, 因此会越变越小。随着递归的深入趋于「消失」，神经网络失去了长程可靠性。这在递归神经网络中是一个较普遍的问题，<strong>对于递归神经网络而言，长程可靠性尤为重要。</strong></p><p>这一问题可通过采用 ReLu 等没有小梯度的激活函数来有效避免。</p><hr><h2 id="25-梯度爆炸问题"><a href="#25-梯度爆炸问题" class="headerlink" title="25. 梯度爆炸问题"></a>25. 梯度爆炸问题</h2><p><strong>梯度爆炸问题与梯度消失问题正好相反，梯度爆炸问题中，激活函数的梯度过大。</strong></p><p>在反向传播过程中，部分节点的大梯度使得他们的权重变得非常大，从而削弱了其他节点对于结果的影响，这个问题可以通过截断（即设置一个梯度允许的最大值）的方式来有效避免。</p><hr><h2 id="电影推荐"><a href="#电影推荐" class="headerlink" title="电影推荐"></a>电影推荐</h2><p>框架：TensorFlow</p><p>使用文本卷积神经网络，并使用<a href="https://grouplens.org/datasets/movielens/"><code>MovieLens</code></a>数据集完成电影推荐的任务。</p><p>代码还没有看懂，还在学习</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscxzf0uyj31ee0nugns.jpg" alt="img"></p><hr><h2 id="电影推荐-模型设计"><a href="#电影推荐-模型设计" class="headerlink" title="电影推荐-模型设计"></a>电影推荐-模型设计</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscy0d9ysj30sg0lcdji.jpg" alt="img"></p><hr><h2 id="电影推荐-文本卷积网络"><a href="#电影推荐-文本卷积网络" class="headerlink" title="电影推荐-文本卷积网络"></a>电影推荐-文本卷积网络</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscy3nty5j31hk0lmjuy.jpg" alt="img"></p><p>图片来自Kim Yoon的论文：<a href="https://arxiv.org/abs/1408.5882"><code>Convolutional Neural Networks for Sentence Classification</code></a></p><hr><h2 id="推荐"><a href="#推荐" class="headerlink" title="推荐"></a>推荐</h2><ol><li>推荐<strong>同类</strong>型的电影 思路是计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。</li><li>推荐<strong>您喜欢</strong>的电影 思路是使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加了些随机选择部分。</li><li>看过这个电影的人<strong>还看了</strong>（喜欢）哪些电影<ul><li>首先选出喜欢某个电影的top_k个人，得到这几个人的用户特征向量。</li><li>然后计算这几个人对所有电影的评分</li><li>选择每个人评分最高的电影作为推荐</li><li>同样加入了随机选择</li></ul></li></ol><hr><h2 id="神经网络-对比"><a href="#神经网络-对比" class="headerlink" title="神经网络 对比"></a>神经网络 对比</h2><p>Keras 强调极简主义,你只需几行代码就能构建一个神经网络。 </p><p>Caffe 的文档非常贫乏，但在计算机视觉领域里， Caffe是无可争议的领导者，它非常稳健，非常快速。</p><p>PyTorch 中的 Tensor 和 Numpy 中的数组可以很方便地进行转换，便于用户的使用 。Caffe2 的代码将全部并入 PyTorch，这也给未来的 PyTorch 赋予了更大的潜力。</p><p>DL4J 的文档写得非常好，里面的文件很清晰，有理论阐述，也有代码案例 。</p><p>Cognitive Toolkit 似乎不是很流行。 </p><p>DSSTNE框架只做一件事 一推荐系统，但它把这件事做到了极致。 DSSTNE 还不是一个足够 成熟的项目，而且它封装得太严密了 (black box)。</p><hr><h2 id="标签预测"><a href="#标签预测" class="headerlink" title="标签预测"></a>标签预测</h2><p>automated machine learning</p><p>paper</p>]]></content>
      
      
      <categories>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二孩问题——Tuesday Changes Every Thing</title>
      <link href="/blog-zh/zh-CN/mathematic/%E4%BA%8C%E5%AD%A9%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Tuesday-changes-every-thing.html"/>
      <url>/blog-zh/zh-CN/mathematic/%E4%BA%8C%E5%AD%A9%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94Tuesday-changes-every-thing.html</url>
      
        <content type="html"><![CDATA[<h2 id="题目："><a href="#题目：" class="headerlink" title="题目："></a>题目：</h2><blockquote><p>If, say, it is known that one of the children is a male born on a Tuesday, what is the probability of the other <em>child</em> to be also male?</p></blockquote><h2 id="分析："><a href="#分析：" class="headerlink" title="分析："></a>分析：</h2><p>什么鬼？周一到周日，每天都可以出生，还会影响孩子另一个孩子性别？</p><p>是的。</p><p>The sample space S = {ff, fm, mf, mm}, with f and m standing for “female: and “male”, respectively. </p><p>Denote these <a href="https://www.cut-the-knot.org/Probability/Dictionary.shtml#elementary">elementary events</a> in sequence A1, A2, A3, A4. </p><p>他们的概率分别是 1/4, 1/4, 1/4, 1/4; P(Aj) = 1/4, j = 1, 2, 3, 4.</p><p>B 是男孩出生在周二的概率.</p><p>P(B|A1) = 0, 因为A1里面没有男孩. </p><p>P(B|A2) = P(B|A3) = 1/7, because in the two cases the one male bear has the probability 1/7 to have been born on any specific day, Tuesday in particular. </p><p>The most interesting case is A4. The following table is suggestive:</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscmwh9fjg3076076q2q.gif" alt="two bears birthdays"></p><p>由图可知， P(B|A4) = 13/49.</p><p>P(A1|B), P(A2|B), P(A3|B), P(A4|B) 的概率分别是 0, 1/7, 1/7, 13/49，且应该被重新调节，已达到和为1. </p><p>按比例放大，得到P(A1|B) = 0, P(A2|B) = P(A3|B) = 1/7 ÷ 27/49 = 7/27,P(A4|B) = 13/49 ÷ 27/49 = 13/27.</p><p>所以周二出生的男孩的同胞为女性的概率为$ \frac{7}{27} \times 2 = \frac{14}{27}$.</p><blockquote><p>因此火神作业将一周7天改为一年12个月，答案应该为：</p><p>$$\frac{\frac{1}{12}\times2}{(\frac{12\times2-1}{12\times12})+\frac{1}{12}\times2} = \frac{24}{47} = 0.5106$$</p></blockquote><h2 id="代码验证："><a href="#代码验证：" class="headerlink" title="代码验证："></a>代码验证：</h2><h3 id="代码："><a href="#代码：" class="headerlink" title="代码："></a>代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python2</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randrange</span><br><span class="line"></span><br><span class="line">results = &#123;</span><br><span class="line"><span class="string">&quot;one_boy_born_tuesday&quot;</span>:&#123;<span class="string">&quot;B&quot;</span>: <span class="number">0</span>, <span class="string">&quot;G&quot;</span>: <span class="number">0</span>&#125;, <span class="string">&quot;one_boy&quot;</span>:&#123;<span class="string">&quot;B&quot;</span>: <span class="number">0</span>, <span class="string">&quot;G&quot;</span>: <span class="number">0</span>&#125;, <span class="string">&quot;first_is_boy&quot;</span>:&#123;<span class="string">&quot;B&quot;</span>: <span class="number">0</span>, <span class="string">&quot;G&quot;</span>: <span class="number">0</span>&#125;, <span class="string">&quot;first_is_boy_born_tuesday&quot;</span>:&#123;<span class="string">&quot;B&quot;</span>: <span class="number">0</span>, <span class="string">&quot;G&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">10000000</span>):</span><br><span class="line">    house = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">        sex = <span class="string">&quot;B&quot;</span> <span class="keyword">if</span> randrange(<span class="number">2</span>) == <span class="number">0</span> <span class="keyword">else</span> <span class="string">&quot;G&quot;</span></span><br><span class="line">        day = randrange(<span class="number">7</span>)</span><br><span class="line">        house.append(&#123;<span class="string">&quot;sex&quot;</span>:sex, <span class="string">&quot;day&quot;</span>:day&#125;)</span><br><span class="line"></span><br><span class="line">    exp = <span class="string">&quot;one_boy_born_tuesday&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (house[<span class="number">0</span>][<span class="string">&quot;sex&quot;</span>] == <span class="string">&quot;B&quot;</span> <span class="keyword">and</span> house[<span class="number">0</span>][<span class="string">&quot;day&quot;</span>] == <span class="number">3</span>):</span><br><span class="line">        results[exp][house[<span class="number">1</span>][<span class="string">&quot;sex&quot;</span>]] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> (house[<span class="number">1</span>][<span class="string">&quot;sex&quot;</span>] == <span class="string">&quot;B&quot;</span> <span class="keyword">and</span> house[<span class="number">1</span>][<span class="string">&quot;day&quot;</span>] == <span class="number">3</span>):</span><br><span class="line">        results[exp][house[<span class="number">0</span>][<span class="string">&quot;sex&quot;</span>]] += <span class="number">1</span>   </span><br><span class="line"></span><br><span class="line">    exp = <span class="string">&quot;one_boy&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (house[<span class="number">0</span>][<span class="string">&quot;sex&quot;</span>] == <span class="string">&quot;B&quot;</span>):</span><br><span class="line">        results[exp][house[<span class="number">1</span>][<span class="string">&quot;sex&quot;</span>]] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> (house[<span class="number">1</span>][<span class="string">&quot;sex&quot;</span>] == <span class="string">&quot;B&quot;</span>):</span><br><span class="line">        results[exp][house[<span class="number">0</span>][<span class="string">&quot;sex&quot;</span>]] += <span class="number">1</span>   </span><br><span class="line"></span><br><span class="line">    exp = <span class="string">&quot;first_is_boy&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (house[<span class="number">0</span>][<span class="string">&quot;sex&quot;</span>] == <span class="string">&quot;B&quot;</span>):</span><br><span class="line">        results[exp][house[<span class="number">1</span>][<span class="string">&quot;sex&quot;</span>]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    exp = <span class="string">&quot;first_is_boy_born_tuesday&quot;</span></span><br><span class="line">    <span class="keyword">if</span> (house[<span class="number">0</span>][<span class="string">&quot;sex&quot;</span>] == <span class="string">&quot;B&quot;</span> <span class="keyword">and</span> house[<span class="number">0</span>][<span class="string">&quot;day&quot;</span>] == <span class="number">3</span>):</span><br><span class="line">        results[exp][house[<span class="number">1</span>][<span class="string">&quot;sex&quot;</span>]] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> results.items():</span><br><span class="line">    <span class="keyword">print</span> </span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;-&quot;</span>*<span class="number">60</span></span><br><span class="line">    <span class="keyword">print</span> k</span><br><span class="line">    <span class="keyword">print</span> v</span><br><span class="line">    <span class="keyword">print</span> <span class="number">1.0</span>*v[<span class="string">&quot;B&quot;</span>]/(v[<span class="string">&quot;G&quot;</span>]+v[<span class="string">&quot;B&quot;</span>])</span><br></pre></td></tr></table></figure><p>对千万个孩子进行分析，结果应该很精准了！</p><h3 id="运行结果："><a href="#运行结果：" class="headerlink" title="运行结果："></a>运行结果：</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">------------------------------------------------------------</span><br><span class="line">one_boy</span><br><span class="line">&#123;&#x27;B&#x27;: 2498175, &#x27;G&#x27;: 5000766&#125;</span><br><span class="line"><span class="number">0.33313703895</span></span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">first_is_boy_born_tuesday</span><br><span class="line">&#123;&#x27;B&#x27;: 356077, &#x27;G&#x27;: 357227&#125;</span><br><span class="line"><span class="number">0.499193892085</span></span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">first_is_boy</span><br><span class="line">&#123;&#x27;B&#x27;: 2498175, &#x27;G&#x27;: 2500593&#125;</span><br><span class="line"><span class="number">0.499758140406</span></span><br><span class="line"></span><br><span class="line">------------------------------------------------------------</span><br><span class="line">one_boy_born_tuesday</span><br><span class="line">&#123;&#x27;B&#x27;: 661673, &#x27;G&#x27;: 714128&#125;</span><br><span class="line"><span class="number">0.480936559866</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>完美！</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference:"></a>Reference:</h2><ol><li><a href="http://www.cut-the-knot.org/Probability/BearBornOnTuesday.shtml">http://www.cut-the-knot.org/Probability/BearBornOnTuesday.shtml</a></li><li><a href="https://math.stackexchange.com/questions/2769616/two-child-probability-paradox-a-nuanced-explanation">https://math.stackexchange.com/questions/2769616/two-child-probability-paradox-a-nuanced-explanation</a></li><li><a href="https://maths.ucd.ie/~plynch/Publications/BIMS-TwoChildParadox.pdf">https://maths.ucd.ie/~plynch/Publications/BIMS-TwoChildParadox.pdf</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> mathematic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mathematic </tag>
            
            <tag> 概率论 </tag>
            
            <tag> probability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无碳小车</title>
      <link href="/blog-zh/zh-CN/%E5%88%B6%E4%BD%9C/%E6%97%A0%E7%A2%B3%E5%B0%8F%E8%BD%A6.html"/>
      <url>/blog-zh/zh-CN/%E5%88%B6%E4%BD%9C/%E6%97%A0%E7%A2%B3%E5%B0%8F%E8%BD%A6.html</url>
      
        <content type="html"><![CDATA[<p><strong>上海交通大学工程训练综合能力竞赛命题——重力势能驱动的S型无碳小车</strong> </p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscuqaifjj30dv09zt9p.jpg" alt="无碳小车"></p><h2 id="竞赛指导思想"><a href="#竞赛指导思想" class="headerlink" title="竞赛指导思想"></a>竞赛指导思想</h2><p>对接“全国大学生工程训练综合能力竞赛”，以“重在实践，鼓励创新”为指导思想，旨在加强大学生工程实践能力、创新意识和合作精神的培养，激发大学生进行科学研究与探索的兴趣，挖掘大学生的创新潜能与智慧，为优秀人才脱颖而出创造良好的条件；推动高等教育人才培养模式和实践教学的改革，不断提高人才培养的质量。通过竞赛活动加强教育与产业、学校与社会、学习与创业之间的联系。</p><p><strong>注：全国大学生工程训练综合能力竞赛是教育部高等教育司发文举办的全国性大学生科技创新实践竞赛活动，同时也是教育部财政部开展的“本科教学质量与教学改革工程”资助竞赛之一，是基于国内各普通高等学校综合性工程训练教学平台，面向全国在校本科生开展的科技创新工程实践活动。竞赛的宗旨是以竞赛为人才培养服务，以竞赛为教育质量助力，以竞赛为创业就业引路。竞赛的方针是基于理论、注重创新，突出能力，强化实践。</strong></p><p>我校为了更好地筹备本次竞赛，现将本次比赛命题如下：</p><h2 id="比赛命题"><a href="#比赛命题" class="headerlink" title="比赛命题"></a>比赛命题</h2><p>本届竞赛命题为“<strong>以重力势能驱动的具有方向控制功能的自行小车</strong>”。</p><p>设计一种小车，驱动其行走及转向的能量是根据能量转换原理，由给定重力势能转换而得到的。该给定重力势能由中心统一提供质量为1Kg的标准砝码（￠50×65 mm，碳钢制作）来获得，要求砝码的可下降高度为400±2mm。标准砝码始终由小车承载，不允许从小车上掉落。图1为小车示意图。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscutm7fjj30px0dv0sy.jpg" alt="img"></p><p>图1  小车示意图</p><p>要求小车在行走过程中完成所有动作所需的能量均由此给定重力势能转换而得，不可以使用任何其他来源的能量。</p><p>要求小车具有转向控制机构，且此转向控制机构具有可调节功能，以适应放有不同间距障碍物的竞赛场地。</p><p>要求小车为三轮结构。具体设计、材料选用及加工制作均由参赛学生自主完成。</p><p>小车在前行时能够自动绕过赛道上设置的障碍物，如图2。障碍物为直径20mm、高200mm的圆棒，沿赛道中线等距离摆放。以小车前行的距离和成功绕障数量来评定成绩。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscv0pbxgj30nr0ca3z2.jpg" alt="img"></p><p>图2  无碳小车在重力势能作用下自动行走示意图</p><h2 id="比赛过程及要求"><a href="#比赛过程及要求" class="headerlink" title="比赛过程及要求"></a>比赛过程及要求</h2><p>参赛队携带制作完成后的小车，加载统一提供的标准砝码，在指定的赛道上进行比赛。赛道宽度为2米，出发端线距第一个障碍及障碍与障碍之间的间距均为1米，小车出发位置自定，但不得超过出发端线和赛道边界线。每队小车运行2次，取2次成绩中的最好成绩。</p><p>小车有效的绕障方法：小车从赛道一侧越过一个障碍后，整体越过赛道中线且障碍物不被撞倒或推出障碍物定位圆；连续运行，直至小车停止。小车有效的运行距离：停止时小车最远端与出发线之间的垂直距离。凡小车走到终点时，记录砝码剩余高度，按照砝码剩余高度同比例分值计入参赛成绩。</p><p>评分标准：每米得2分，测量读数精确到毫米；每成功绕过1个障碍物得8分，以车体投影全部越过赛道中线为判据。1次绕过多个障碍物时只算1个；多次绕过同1个障碍物只算1个；障碍物被撞倒或推开均不得分。</p><p>按照上述算分办法计算出每个参赛队得分，其小车行走实际成绩为：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscv6rkv4j30dv09kq48.jpg" alt="无碳小车"></p><h2 id="奖项分配"><a href="#奖项分配" class="headerlink" title="奖项分配"></a>奖项分配</h2><p>竞赛组委会按照公平、公正的原则对全体参赛学生的成绩进行校核，按照各赛项奖项比例对获奖的学生予以表彰。竞赛设立一等奖、二等奖，获奖名额分别为全部参赛学生的20%和30%。</p><h2 id="获奖名单"><a href="#获奖名单" class="headerlink" title="获奖名单"></a>获奖名单</h2><p><a href="http://engtc.sjtu.edu.cn/model/twogradepage/newsdetail.aspx?id=831&columnid=66">上海交通大学学生创新中心</a>于2018年6月17日上午举行了2018年第九届上海交通大学大学生工程训练综合能力竞赛暨第八届上海市大学生工程训练综合能力竞赛校内选拔赛，根据比赛规则，现将现将本次校内选拔赛获奖名单如下：</p><p>一等奖：<code>熊磊 孙长江 曾昱程</code>、<code>邹碧铖 潘雨婷 彭桢 胡炳城</code>、<code>丁东宇 陈晨 邵云翥</code>、<code>陈轶钦 吴焜 李寅灏</code></p><p>二等奖：花睿 王一品 朱子扬、廖世扬 张爰思 陶思航、尹鹏程 周群超 罗天、黄鹏升 王浩宇 郑华彬、丁雯怡 王玉冰 韦玉哲、王康昊 程吉 地里亚尔·艾斯合尔</p>]]></content>
      
      
      <categories>
          
          <category> 制作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 制作 </tag>
            
            <tag> 无碳小车 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>平衡机器人-平衡车</title>
      <link href="/blog-zh/zh-CN/%E5%88%B6%E4%BD%9C/%E5%B9%B3%E8%A1%A1%E6%9C%BA%E5%99%A8%E4%BA%BA-%E5%B9%B3%E8%A1%A1%E8%BD%A6.html"/>
      <url>/blog-zh/zh-CN/%E5%88%B6%E4%BD%9C/%E5%B9%B3%E8%A1%A1%E6%9C%BA%E5%99%A8%E4%BA%BA-%E5%B9%B3%E8%A1%A1%E8%BD%A6.html</url>
      
        <content type="html"><![CDATA[<h2 id="材料准备："><a href="#材料准备：" class="headerlink" title="材料准备："></a>材料准备：</h2><p>所用芯片如下图所示：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscp28tlej30u0140aj1.jpg" alt="IMG_20170824_203455.jpg"></p><h2 id="制作过程："><a href="#制作过程：" class="headerlink" title="制作过程："></a>制作过程：</h2><p>马达连接：将两个减速电机用热熔胶粘在一起，如下图所示。箭头所指为热熔胶。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscp8o0d7j315b0u0mz6.jpg" alt="IMG_20170821_192515.png"></p><p>将减速电机装上轮胎与固定件：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscpbmhomj31400u0n2t.jpg" alt="IMG_20170824_122536_1503592919821.jpg"></p><p>将马达连在亚克力板上：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscpef0ufj30u0140q7c.jpg" alt="IMG_20170824_122901.jpg"></p><p>在这层以上再装一层。注意将六轴陀螺仪6050装在六角铜柱与第二层夹板之间。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscpfdoxqj31400u078j.jpg" alt="IMG_20170824_130000_1503592918419.jpg"></p><p>在第二层夹板上安装电池支架。注意将连接6050的电线掰弯一点，再将电线从铜柱后方绕过去。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscph7o05j30u0140dna.jpg" alt="IMG_20170824_210045.jpg"></p><p>将减速电机电线接在电机驱动板上。我用的是一块最常用小红片。这个板子需要连接4个pwm接口。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscpnr3r9j30u01400xz.jpg" alt="IMG_20170824_210318.jpg"></p><p>再焊接一个蓝牙模块所需用到的支架。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscpoq0ajj30u0140jv3.jpg" alt="IMG_20170824_212013.jpg"></p><p>再制作一个总线模块支架。这个支架达到将两个总线控制模块并联的目的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscprkkbaj30u01djwk9.jpg" alt="IMG_20170824_210336.jpg"></p><p>在 Arduino nano 的脚上焊上洞洞板，贴上贴纸并且标出一些引脚的特殊定义。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscputi6zj319c0u00yu.jpg" alt="IMG_20170824_210355.jpg"></p><p>将 Arduino 用热熔胶粘在电池支架上，并在支架里面放入电池。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscpx8ey7j30u0140gqm.jpg" alt="IMG_20170824_213748_1503592902708.jpg"></p><p>制作开关模块，接线，注意线需要穿过带洞长方形板。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscq41vyfj30u0140q7c.jpg" alt="IMG_20170830_095522.jpg"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscq8lcsnj30u0140q7c.jpg" alt="IMG_20170830_095451.jpg"></p><p>制作机器人的“脸”。将OLED显示屏粘在亚克力板上，并且粘上两个小耳朵。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqalup6j30u01400xs.jpg" alt="IMG_20170824_213600_1503592902919.jpg"></p><p>反面照片：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqdjffwj31400u0n22.jpg" alt="IMG_20170824_213546_1503592903384.jpg"></p><p>安装在总线支架上，然后测试一下。显示“平衡小车启动中”七个字，清晰明亮。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqeg0rzj30u0140te4.jpg" alt="IMG_20170830_102928.jpg"></p><p>将开关固定。1为整机启动开关；2为充电开关；3为充电插口。这儿使用了专用3.7V锂电池充电器，所以不要加充电模块。1、2 不要在充电时同时开启。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqftt4lj30u0140790.jpg" alt="IMG_20170830_102948.jpg"></p><p>在底盘处安装一枚彩色LED。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqi3rhnj30u0140djj.jpg" alt="IMG_20170830_145615.jpg"></p><p>在机器人腰部安装一个大电容，这个电容和电池并联。如果没有这个电容小车将不能急刹也不能快速加速。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqjmzbzj30u00y6ae0.jpg" alt="IMG_20170830_150022.jpg"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqkghd7j30u0140djj.jpg" alt="IMG_20170830_163247.jpg"></p><p>基本安装完毕后开始接线，注意要用热熔胶将线胶在洞洞板上，防止断裂。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqn88m7j30u0140wkg.jpg" alt="IMG_20170830_163226.jpg"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqoaj08j30u0140jva.jpg" alt="IMG_20170830_163233.jpg"></p><p>接好线后将带洞的长方形亚克力板用热熔胶固定。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqpp4hzj30u0140q7c.jpg" alt="IMG_20170830_185807.jpg"></p><p>然后将头部插入总线支架。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqsj3vsj30u01400zg.jpg" alt="IMG_20170830_185831.jpg"></p><p>背部插线操作相同，注意将PDS9930距离传感器安装好。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscqtspiaj30u014078l.jpg" alt="IMG_20170830_185949.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> 制作 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 制作 </tag>
            
            <tag> 平衡车 </tag>
            
            <tag> arduino </tag>
            
            <tag> 陀螺仪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>日本游学记事</title>
      <link href="/blog-zh/zh-CN/travel/%E6%97%A5%E6%9C%AC%E6%B8%B8%E5%AD%A6%E8%AE%B0%E4%BA%8B.html"/>
      <url>/blog-zh/zh-CN/travel/%E6%97%A5%E6%9C%AC%E6%B8%B8%E5%AD%A6%E8%AE%B0%E4%BA%8B.html</url>
      
        <content type="html"><![CDATA[<h1 id="日本🇯🇵"><a href="#日本🇯🇵" class="headerlink" title="日本🇯🇵"></a>日本🇯🇵</h1><h2 id="日本大学"><a href="#日本大学" class="headerlink" title="日本大学"></a>日本大学</h2><p>2018年一月的寒假，到日本游学。日本的大学都很小，上智大学更是只有三栋教学楼，早稻田大学稍微大一些。早稻田大学作为日本人才基地，是十分漂亮的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctd7n0tj31440u0wob.jpg" alt="早稻田大学 1"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscthxlnkj30u0144wrv.jpg" alt="早稻田大学 2"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctjomz1j31440u0wob.jpg" alt="钟楼门前是学生堆的雪人"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctlmq43j30u014411j.jpg" alt="游泳馆前积雪未消"></p><h2 id="游学"><a href="#游学" class="headerlink" title="游学"></a>游学</h2><p>游学一事，游在前，自然看遍日本山河。日本国土面积不大，注定了景观都是小家碧玉型的。而日本在这方面又做到了极致。</p><p>日本是一个注重技艺的国家，而浮世绘作为日本最受欢迎的出口文化之一，也作为抽象画派构思来源，获得了世界的认可。</p><h2 id="日本东京博物馆"><a href="#日本东京博物馆" class="headerlink" title="日本东京博物馆"></a>日本东京博物馆</h2><p>日本空气质量比起湖南是好多了。整整一个月就没见过不是蓝色的天空。下面两张日本东京博物馆的图片是用小米note3拍摄的，没做任何后期处理。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctog0moj31440u07gg.jpg" alt="大阪国立博物馆 馆二"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctptmvoj31440u0k35.jpg" alt="大阪国立博物馆 馆一"></p><h3 id="展品：蛤仙图"><a href="#展品：蛤仙图" class="headerlink" title="展品：蛤仙图"></a>展品：蛤仙图</h3><p>作为上海交通大学学生，看到这幅长卷我就走不动了。</p><p>拍下了一些片段，全卷是一个故事。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctrbi39j31kw0osah5.jpg" alt="蛤官上任"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctvcn00j30u0144gqr.jpg" alt="蛤兔相争"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gisctz4itxj31kw0osah5.jpg" alt="仙蛤牵野猪"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscu364v0j31kw0omdn6.jpg" alt="仙蛤戏兔"></p><p><img src="../images/%E6%97%A5%E6%9C%AC%E6%B8%B8%E5%AD%A6%E8%AE%B0%E4%BA%8B/%E4%BB%99%E8%9B%A4%E8%BF%BD%E9%87%8E%E7%8C%B4.jpg" alt="仙蛤追野猴"></p><p><img src="https://ws1.sinaimg.cn/large/0069RVTdly1fu05hju38nj31kw0o1k1p.jpg" alt="一秒上仙"></p>]]></content>
      
      
      <categories>
          
          <category> travel </category>
          
      </categories>
      
      
        <tags>
            
            <tag> travel </tag>
            
            <tag> Japan </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>叹晚自习</title>
      <link href="/blog-zh/zh-CN/%E8%AF%97%E6%AD%8C/%E5%8F%B9%E6%99%9A%E8%87%AA%E4%B9%A0.html"/>
      <url>/blog-zh/zh-CN/%E8%AF%97%E6%AD%8C/%E5%8F%B9%E6%99%9A%E8%87%AA%E4%B9%A0.html</url>
      
        <content type="html"><![CDATA[<p>荧光条条明如昼，白光漫漫洒窗牖。<br>细针落地犹可闻，红日一去万音休。<br>千百头颅伏案上，不屈脊梁在此佝。<br>此般姿态何使得？但愿他日拔头筹！</p><hr><p><em>——谨以此诗纪念一去不回的高中生活！纪念那热血的奋斗、那纯洁的感情和那永远写不完的模拟！</em></p><p>——还记得那时候停了电，有人用手机照明，有人用小电筒。更多的人什么都没有。而我买了一个当时少见的USB头的充电小手电。每次停电都暗自高兴——我真有远见，买了小手电！</p><p>——还记得夏天天热，我买了一个车载小风扇，带了一个12V硫酸铅蓄电池，固定在我的大钢瓶上面，享受着一个人的清凉。</p><p>——还记得下课后和臻姐一起，跑到主席台上，和十来个大妈一起跳舞。男的女的、会跳的不会跳的都尽情的扭动着自己。广场舞也就成了青春的一部分……</p><blockquote><p>回</p><p>不</p><p>去</p><p>的</p><p>高</p><p>中</p><p>生</p><p>活</p><p>。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscu7s5d9j30b4077jrs.jpg" alt="cb1e4e0807ecc7ede608081b9baa929b"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscue7vndj30bt06owep.jpg" alt="a0cf87728a9f37fb6d4f3fb2e44c40d2"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscuf2i2nj30dw0afq3u.jpg" alt="d69faa0ed750dd06b4df65d4668589f1"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscum498zj30ci08cwfi.jpg" alt="df7634eed724865a6c01783425ebfd88"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1giscuov2u7j30m80go0u6.jpg" alt="8d4202409d4cb8e9e82e668f63e43091"></p>]]></content>
      
      
      <categories>
          
          <category> 诗歌 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 诗歌 </tag>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
